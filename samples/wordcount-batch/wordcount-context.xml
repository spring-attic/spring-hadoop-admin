<?xml version="1.0" encoding="UTF-8"?>
<beans:beans xmlns="http://www.springframework.org/schema/hadoop"
	xmlns:beans="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
    xmlns:batch="http://www.springframework.org/schema/batch"
    xmlns:p="http://www.springframework.org/schema/p"
    xmlns:context="http://www.springframework.org/schema/context"
	xsi:schemaLocation="http://www.springframework.org/schema/batch https://www.springframework.org/schema/batch/spring-batch-2.1.xsd
		http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd
		http://www.springframework.org/schema/hadoop https://www.springframework.org/schema/hadoop/spring-hadoop.xsd
		http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context-3.0.xsd">

    <context:property-placeholder location="hadoop.properties"
			ignore-resource-not-found="true" ignore-unresolvable="true" />

 	
 	<configuration>	   
		fs.default.name=${hd.fs}
		mapred.job.tracker=${mapred.job.tracker}
	</configuration>
	
	<batch:job id="wordcount-withscript-job">
		<batch:step id="import" next="wordcount">
			<batch:tasklet ref="script-tasklet"/>
		</batch:step>
		<batch:step id="wordcount">
			<batch:tasklet ref="wordcount-tasklet" />
		</batch:step>
	</batch:job>

	<tasklet id="wordcount-tasklet" job-ref="wc-job"/>

    <job id="wc-job"
              input-path="/user/hadoop/input"
              output-path="#{ T(org.springframework.data.hadoop.util.PathUtils).format('/user/hadoop/output/%1$tY/%1$tm/%1$td/%1$tH/%1$tM/%1$tS')}"
              mapper="org.apache.hadoop.examples.WordCount.TokenizerMapper"
              reducer="org.apache.hadoop.examples.WordCount.IntSumReducer"
              validate-paths="false"
              jar="${wordcount.jar.path}"
              />


	<script-tasklet id="script-tasklet">
  	   <script language="groovy">
  	   	// 'hack' default permissions to make Hadoop work on Windows
		if (System.getProperty("os.name").startsWith("Windows")) {
			// 0655 = -rwxr-xr-x
			 org.apache.hadoop.mapreduce.JobSubmissionFiles.JOB_DIR_PERMISSION.fromShort((short) 0655);
			 org.apache.hadoop.mapreduce.JobSubmissionFiles.JOB_FILE_PERMISSION.fromShort((short) 0655);
		}

  	   
		inputPath = "/user/hadoop/input"
		outputPath = "/user/hadoop/output"	
		if (fsh.test(inputPath)) {
		  fsh.rmr(inputPath)
		}
		if (fsh.test(outputPath)) {
		  fsh.rmr(outputPath)
		}

		// copy using the streams directly (to be portable across envs)
		inStream = cl.getResourceAsStream("data/nietzsche-chapter-1.txt")
		org.apache.hadoop.io.IOUtils.copyBytes(inStream, fs.create(inputPath), cfg)
	  </script>	
    </script-tasklet>


</beans:beans>
